{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juhQWH6wahXw",
        "outputId": "f9052ddd-871f-4ea5-faf3-d62f1adba6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "nV2w3EXNIl3J",
        "outputId": "a14229f2-ec79-4009-a2a4-7992314c69ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23s038\u001b[0m (\u001b[33mcs23s038-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "evH6-lD5DWUF",
        "outputId": "7876aea2-3669-4ea5-d1a3-8df3b68efb5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_165709-tf9rlxrs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/tf9rlxrs' target=\"_blank\">stoic-frost-300</a></strong> to <a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1' target=\"_blank\">https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/tf9rlxrs' target=\"_blank\">https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/tf9rlxrs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/tf9rlxrs?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e8b316e2950>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "wandb.init(project=\"UDIT_DL_1\", entity=\"cs23s038-iit-madras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCmJWyMxI61-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ed7913-a424-4be2-b0c3-fe49b9198baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ca5xrgqq\n",
            "Sweep URL: https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/sweeps/ca5xrgqq\n"
          ]
        }
      ],
      "source": [
        "# sweep_config = {\n",
        "#     \"method\": \"bayes\",\n",
        "#     \"metric\": {\n",
        "#         \"name\": \"val_accuracy\",\n",
        "#         \"goal\": \"maximize\"\n",
        "#     },\n",
        "#     \"parameters\": {\n",
        "#         \"epochs\": {\"values\": [5, 10]},\n",
        "#         \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "#         \"h_layers\": {\"values\": [3, 4, 5]},\n",
        "#         \"neurons\": {\"values\": [32, 64, 128]},\n",
        "#         \"optimizer\": {\"values\": ['sgd', 'mgd', 'nesterov', 'rmsprop', 'adam', 'nadam']},\n",
        "#         \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "#         \"activation\": {\"values\": ['sigmoid', 'tanh', 'relu']}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# # Initialize Sweep\n",
        "# sweep_id = wandb.sweep(sweep=sweep_config, project=\"UDIT_DL_1\")\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [10]},\n",
        "        \"learning_rate\": {\"values\": [1e-4]},\n",
        "        \"h_layers\": {\"values\": [4]},\n",
        "        \"neurons\": {\"values\": [128]},\n",
        "        \"optimizer\": {\"values\": ['adam']},\n",
        "        \"batch_size\": {\"values\": [64]},\n",
        "        \"activation\": {\"values\": ['tanh']}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize Sweep\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"UDIT_DL_1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgsLmKZMpyH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt8dmitqMs_4"
      },
      "source": [
        "###Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lE5PJPWMcKJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.where(np.asarray(x) > 0, x, 0)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    return e_x/e_x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4IZu4f2hCP"
      },
      "source": [
        "###Loss Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EE-A_Pw2kMi"
      },
      "outputs": [],
      "source": [
        "#cross-entropy\n",
        "def cross_entropy_loss(y, y_hat, i): #y_hat is a (10 * 1) matrix containing probabilities corresponding to each class\n",
        "  return -np.log(y_hat[y[i]]) #y[i] is the true label number(say y[i] --> 5) --> -(1 * log(0.8)) rest 0, and hence only that term will be non-zero in cross entropy\n",
        "\n",
        "#squared-error\n",
        "def squared_error(y, y_hat, i):\n",
        "  e_l = np.zeros((y_hat.shape[0], 1))\n",
        "  e_l[y[i]] = 1;\n",
        "\n",
        "  #y_hat[y[i]] = (1-y_hat[y[i]])**2\n",
        "  #return np.sum(np.square(y_hat))\n",
        "  loss = np.sum((y_hat - e_l) ** 2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x23K52DJMwJu"
      },
      "source": [
        "###Layer Class : parameters initialization for each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSYeILJNMvIV"
      },
      "outputs": [],
      "source": [
        "class Layer:  # W, b, act, d_act, dW, db ---> each layer stores then future adds da and dh\n",
        "\n",
        "    activationFunc = { #types of activation functions and there grad\n",
        "        'tanh': (tanh, d_tanh),\n",
        "        'sigmoid': (sigmoid, d_sigmoid),\n",
        "        'relu' : (relu, d_relu),\n",
        "        'softmax' : (softmax, None)\n",
        "    }\n",
        "\n",
        "    def __init__(self, inputs, neurons, activation):\n",
        "\n",
        "        #Xavier initialization sets the initial values of weights in a way that prevents gradients from vanishing or exploding during training.\n",
        "        np.random.seed(33)  #ensures that the sequence of random numbers generated is reproducible.\n",
        "        sd = np.sqrt(2 / float(inputs + neurons)) #calculates the standard deviation (sd) used for Xavier initialization of the weights.\n",
        "        self.W = np.random.normal(0, sd, size=(neurons, inputs))  #initializes the weights (W) of the layer using a normal distribution with mean 0 and standard deviation sd --> shape of output array (neurons * inputs)\n",
        "        self.b = np.zeros((neurons, 1)) #init bias to 0 ---> shape(neurons * 1)\n",
        "        self.act, self.d_act = self.activationFunc.get(activation) #activation-func and diff_act-func is taken from argument.\n",
        "        self.dW = 0 #gradients of the loss function with respect to the weights and biases of the layer, respectively\n",
        "        self.db = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnulycSKMLB6"
      },
      "source": [
        "### Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK_UOVYHs3Vv"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(h, layers):\n",
        "  m = len(layers) #all layers present ---> input + hidden + output\n",
        "\n",
        "  layers[0].a = np.dot(layers[0].W, h) #first layer pre-activation\n",
        "  layers[0].h = layers[0].act(layers[0].a) #first layer activation\n",
        "\n",
        "  for j in range(1, m-1):\n",
        "    layers[j].a = np.dot(layers[j].W, layers[j-1].h) #hidden layers pre-activation\n",
        "    layers[j].h = layers[j].act(layers[j].a) #hidden layers activation\n",
        "\n",
        "  j+=1\n",
        "  layers[j].a = np.dot(layers[j].W, layers[j-1].h) #last layers pre-activation\n",
        "  layers[j].h = softmax(layers[j].a) #output layer activation using softmax fucntion ---> returns probability\n",
        "  return layers[m-1].h #returns the probabilty given by softmax function of each class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J1L5aJ3MNvW"
      },
      "source": [
        "###Backward_propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib1ZyluaRYnz"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(l, y_hat, layers, inp): # l ---> label number of true class\n",
        "\n",
        "  #one-hot vector\n",
        "  e_l = np.zeros((y_hat.shape[0], 1)) #init a vector of size (y_hat * 1) all set to 0\n",
        "  e_l[l] = 1 #set 1 corresponding to the true class label ---> true one-hot encoded vector\n",
        "\n",
        "  layers[len(layers)-1].da = -(e_l - y_hat) #gradient w.r.t activation of last layer (a_L) cross-entropy\n",
        "  #layers[len(layers)-1].da = np.multiply(2 * np.multiply(y_hat, (1 - y_hat)), (y_hat - e_l)) #gradient w.r.t activation of last layer (a_L) cross-entropy\n",
        "\n",
        "  for j in range(len(layers)-1, 0, -1): #grads from L-1 to 1 layer\n",
        "\n",
        "    layers[j].dW += np.dot(layers[j].da, (layers[j-1].h).T)\n",
        "    layers[j].db += layers[j].da\n",
        "\n",
        "    layers[j-1].dh = np.dot((layers[j].W).T, layers[j].da)\n",
        "    layers[j-1].da = np.multiply(layers[j-1].dh, layers[j-1].d_act(layers[j-1].a))\n",
        "\n",
        "  layers[0].dW += np.dot(layers[0].da, inp.T)\n",
        "  layers[0].db += layers[0].da\n",
        "\n",
        "  return layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Lo597P3hI8"
      },
      "source": [
        "###SGD / Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acn4nh9X3gIQ"
      },
      "outputs": [],
      "source": [
        "def sgd(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    m = x_train.shape[0] #60,000\n",
        "    costs = []\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      cost = 0\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1) #each 28 * 28 image is converted into 784 * 1 column vector.\n",
        "\n",
        "        #Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers) #each column(image) is passed as input along with the layers.\n",
        "\n",
        "        #Calulate training loss\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1)) # wrt i-th datapoint ---> y_train[i] --> true class label, h --> prob vector\n",
        "\n",
        "        #mini-batch gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - learning_rate * layer.dW/batch_size # W for next iteration is current W - eta * dW\n",
        "            layer.b = layer.b - learning_rate * layer.db/batch_size\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "      costs.append(cost/m) #normalised cross-entropy loss after each epoch\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers) #Run the trained model after every epoch in validation data at once ---> entire [6000 * 784].T matrix is passed to forward prop at once, it output ---> [10 * 6000] y_hat matrix\n",
        "\n",
        "      val_loss = 0\n",
        "\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i) #check the validation loss after every epoch\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0) # takes/assigns it to that class which have maximum probabity ----- 1 * 6000\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0] #calculate validation accuracy where every we made a correct pridiction upon total points after every epoch\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcWfy6JBDyJ-"
      },
      "source": [
        "###Momentum Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vksSBdq-D1cC"
      },
      "outputs": [],
      "source": [
        "def mgd(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:  #inititalize w, b to 0\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "\n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #momentum gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size #current delta is some gamma times previous history plus current delta\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers)\n",
        "\n",
        "      val_loss = 0\n",
        "\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0)\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0]\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp6A3a8gnoSe"
      },
      "source": [
        "###Nesterov Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBdvwjl3nljy"
      },
      "outputs": [],
      "source": [
        "def nesterov(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "\n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0: #first move by history and then calculate grad at this point and then move accordingly\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.update_W #moved by history(momentum)\n",
        "            layer.b = layer.b - gamma * layer.update_b\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1)) #calculate grad at this moved point\n",
        "\n",
        "        #nesterov gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size #final update\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers)\n",
        "\n",
        "      val_loss = 0\n",
        "\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0)\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0]\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG_GGFWjXaRO"
      },
      "source": [
        "###RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGBnrjWeXcpt"
      },
      "outputs": [],
      "source": [
        "def rmsprop(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    epsilon, beta = 1e-8, 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "\n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #rmsprop gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = beta*layer.update_W + (1-beta)*(layer.dW/batch_size)**2\n",
        "            layer.update_b = beta*layer.update_b + (1-beta)*(layer.db/batch_size)**2\n",
        "\n",
        "            layer.W = layer.W - (learning_rate / np.sqrt(layer.update_W + epsilon)) * (layer.dW/batch_size)\n",
        "            layer.b = layer.b - (learning_rate / np.sqrt(layer.update_b + epsilon)) * (layer.db/batch_size)\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers)\n",
        "\n",
        "      val_loss = 0\n",
        "\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0)\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0]\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Dgl76nu7Pd"
      },
      "source": [
        "###Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKZU2d9Su9Vk"
      },
      "outputs": [],
      "source": [
        "def adam(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
        "    t = 0\n",
        "\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "\n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #adam gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          t+=1\n",
        "\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.m_W = beta1 * layer.m_W + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_b = beta1 * layer.m_b + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.v_W = beta2 * layer.v_W + (1-beta2)*((layer.dW/batch_size))**2\n",
        "            layer.v_b = beta2 * layer.v_b + (1-beta2)*((layer.db/batch_size))**2\n",
        "\n",
        "            layer.m_W_hat = layer.m_W/(1-math.pow(beta1, t))\n",
        "            layer.m_b_hat = layer.m_b/(1-math.pow(beta1, t))\n",
        "\n",
        "            layer.v_W_hat = layer.v_W/(1-math.pow(beta2, t))\n",
        "            layer.v_b_hat = layer.v_b/(1-math.pow(beta2, t))\n",
        "\n",
        "            layer.W = layer.W - (learning_rate/np.sqrt(layer.v_W_hat + epsilon))*layer.m_W_hat\n",
        "            layer.b = layer.b - (learning_rate/np.sqrt(layer.v_b_hat + epsilon))*layer.m_b_hat\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0)\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0]\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9L5Efq9zs8w"
      },
      "source": [
        "###NAdam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdwVLM1fzu7V"
      },
      "outputs": [],
      "source": [
        "def nadam(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
        "    gamma = 0.9\n",
        "    t = 0\n",
        "\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "\n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.m_W\n",
        "            layer.b = layer.b - gamma * layer.m_b\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #adam gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          t+=1\n",
        "\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.m_W = beta1 * layer.m_W + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_b = beta1 * layer.m_b + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.v_W = beta2 * layer.v_W + (1-beta2)*((layer.dW/batch_size))**2\n",
        "            layer.v_b = beta2 * layer.v_b + (1-beta2)*((layer.db/batch_size))**2\n",
        "\n",
        "            layer.m_W_hat = layer.m_W/(1-math.pow(beta1, t))\n",
        "            layer.m_b_hat = layer.m_b/(1-math.pow(beta1, t))\n",
        "\n",
        "            layer.v_W_hat = layer.v_W/(1-math.pow(beta2, t))\n",
        "            layer.v_b_hat = layer.v_b/(1-math.pow(beta2, t))\n",
        "\n",
        "            layer.m_dash_W = beta1 * layer.m_W_hat + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_dash_b = beta1 * layer.m_b_hat + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.W = layer.W - (learning_rate/np.sqrt(layer.v_W_hat + epsilon))*layer.m_dash_W\n",
        "            layer.b = layer.b - (learning_rate/np.sqrt(layer.v_b_hat + epsilon))*layer.m_dash_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      #predict on validation data\n",
        "      prediction = forward_propagation(x_val.T, layers)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i in range(len(y_val)):\n",
        "        val_loss += cross_entropy_loss(y_val, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "      val_loss = val_loss/len(y_val)\n",
        "      prediction = prediction.argmax(axis=0)\n",
        "      val_accuracy =  np.sum(prediction == y_val)/y_val.shape[0]\n",
        "\n",
        "      #wandb logs\n",
        "      wandb.log({\"epoch\": epoch, \"train_loss\": costs[len(costs)-1], \"val_accuracy\": val_accuracy, \"val_loss\": val_loss})\n",
        "\n",
        "      print(\"-----------------epoch \"+str(epoch)+\"-----------------\")\n",
        "      print(\"Training loss: \", cost/m)\n",
        "      print(\"Validation accuracy: \", val_accuracy)\n",
        "      print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    return costs, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os37IrNr-ClW"
      },
      "source": [
        "###Putting all togather:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN3FKWWGNkN6"
      },
      "source": [
        "###Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sRz9nk6NgVa"
      },
      "outputs": [],
      "source": [
        "def optimizor(layers, optimizer, epochs, learning_rate, x_train, y_train, x_val, y_val, batch_size):\n",
        "\n",
        "  if optimizer == \"sgd\":\n",
        "    return sgd(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  elif optimizer == \"mgd\":\n",
        "    return mgd(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  elif optimizer == \"nesterov\":\n",
        "    return nesterov(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  elif optimizer == \"rmsprop\":\n",
        "    return rmsprop(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  elif optimizer == \"adam\":\n",
        "    return adam(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  elif optimizer == \"nadam\":\n",
        "    return nadam(epochs, layers, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "  else:\n",
        "    print(\"No optimization algorithm named \"+optimizer+\" found\")\n",
        "    return \"Error\", \"Error\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbjCtQVxNYwI"
      },
      "source": [
        "###Function to Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXkMp1rLNbaj"
      },
      "outputs": [],
      "source": [
        "def predict(input, y, layers): #After the model is trained do one pass of forward pass in test data and note loss\n",
        "\n",
        "  prediction = forward_propagation(input, layers)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "\n",
        "  for i in range(len(y)):\n",
        "    loss += cross_entropy_loss(y, prediction[:, i].reshape(10,1), i)\n",
        "\n",
        "  prediction = prediction.argmax(axis=0)\n",
        "  accuracy = np.sum(prediction == y)/y.shape[0]\n",
        "\n",
        "  return prediction, accuracy, loss/len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcVmssSHNp78"
      },
      "source": [
        "###Import dataset and putting in appropriate format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gFb9PXW50Fc",
        "outputId": "3ea63e66-6b5f-40ff-a72e-49f4c2ffa048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZfY5xKEnUBY"
      },
      "source": [
        "**Normalizing the DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxKqgiBfFg0R",
        "outputId": "77fc8b92-03b0-48f0-ed99-ef30443a9dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "x_train_org = x_train_org / 255.0\n",
        "x_test_org = x_test_org / 255.0\n",
        "\n",
        "print(y_test_org.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5LBFhdQ5vIV"
      },
      "source": [
        "###Display images corresponding to each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHXIVJhSiRo_"
      },
      "outputs": [],
      "source": [
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "# Initialize an empty dictionary to store one image per class\n",
        "sample_images_per_class = []\n",
        "\n",
        "# Iterate through each class\n",
        "for i in range(10):\n",
        "    # Find the index of the first occurrence of class i\n",
        "    idx = np.where(y_train_org == i)[0][0]\n",
        "    # Add one image from class i to the dictionary\n",
        "    sample_images_per_class.append(wandb.Image(x_train_org[idx], caption=class_labels[i]))\n",
        "\n",
        "# Log the dictionary containing one image per class to W&B\n",
        "wandb.log({\"sample_images\": sample_images_per_class})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9eUrqrAU1N4"
      },
      "source": [
        "####Flattening the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVrG8aF6A6HY",
        "outputId": "4db31b9a-aed6-4906-f949-284fce6f0310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "x_train_temp = x_train_org.reshape(x_train_org.shape[0], -1)  #reshapes x-train to 60000 * 784 ---> .shape return [no. of rows, no. of cols]\n",
        "y_train_temp = y_train_org\n",
        "x_test = x_test_org.reshape(x_test_org.shape[0], -1) #reshape keeps the numbers of rows(60000) same and no of columns are infered based on shape of data.\n",
        "y_test = y_test_org\n",
        "\n",
        "print(x_train_temp.shape)\n",
        "print(y_train_temp.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykV4aRkBU67b"
      },
      "source": [
        "####Splliting dataset into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdiEOLK1TryC",
        "outputId": "6b05c1e6-f377-4ca5-8a99-e205559996c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000, 784)\n",
            "(54000,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_temp, y_train_temp, test_size=0.1, random_state=33)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blyiJCdN4RCt"
      },
      "source": [
        "###Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GihUlSgl4Qll"
      },
      "outputs": [],
      "source": [
        "def model_train(epochs, learning_rate, neurons, h_layers, activation, batch_size, optimizer, x_train, y_train, x_val, y_val):\n",
        "\n",
        "  layers= [Layer(x_train.shape[1], neurons, activation)]\n",
        "  for _ in range(0, h_layers-1):\n",
        "    layers.append(Layer(neurons, neurons, activation))\n",
        "  layers.append(Layer(neurons, 10, 'softmax'))\n",
        "\n",
        "  costs, layers = optimizor(layers, optimizer, epochs, learning_rate, x_train, y_train, x_val, y_val, batch_size)\n",
        "\n",
        "  output_test, accuracy_test, test_loss = predict(x_test.T, y_test, layers)\n",
        "\n",
        "  #wandb.log({\"accuracy\": accuracy_test})\n",
        "  #wandb.log({\"Testing loss\": test_loss})\n",
        "\n",
        "  print(\"----------------------------------\")\n",
        "  print(\"Test accuracy: \", accuracy_test)\n",
        "  print(\"Test loss: \", test_loss)\n",
        "\n",
        "  return output_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfm6cD7aM9nu"
      },
      "source": [
        "###Train Model with wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrKrKnZSKxYa"
      },
      "outputs": [],
      "source": [
        "def model_train_wandb(config=None):\n",
        "    \"\"\"Function to train the model using WandB Sweep parameters.\"\"\"\n",
        "\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config  # Get hyperparameter values from WandB\n",
        "\n",
        "        # Generate a unique run name based on hyperparameters\n",
        "        run_name = f\"-hl{config.h_layers}-bs{config.batch_size}-ac_{config.activation}\"\n",
        "        wandb.run.name = run_name\n",
        "\n",
        "        # Define layers for the neural network\n",
        "        layers = [Layer(x_train.shape[1], config.neurons, config.activation)]\n",
        "        for _ in range(config.h_layers - 1):\n",
        "            layers.append(Layer(config.neurons, config.neurons, config.activation))\n",
        "        layers.append(Layer(config.neurons, 10, 'softmax'))\n",
        "\n",
        "        # Train the model\n",
        "        costs, layers = optimizor(\n",
        "            layers, config.optimizer, config.epochs, config.learning_rate,\n",
        "            x_train, y_train, x_val, y_val, config.batch_size\n",
        "        )\n",
        "\n",
        "        # Evaluate on test data\n",
        "        output_test, accuracy_test, test_loss = predict(x_test.T, y_test, layers)\n",
        "\n",
        "        # Log results\n",
        "        print(\"----------------------------------\")\n",
        "        print(\"Test accuracy:\", accuracy_test)\n",
        "        print(\"Test loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QowHkgFpjTmS"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function=model_train_wandb, count=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "US_OXfa7mfM5",
        "outputId": "c0044a43-431f-48a3-a863-98b6d0fd9e8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-galaxy-15</strong> at: <a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/ljxmi4ab' target=\"_blank\">https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1/runs/ljxmi4ab</a><br> View project at: <a href='https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1' target=\"_blank\">https://wandb.ai/cs23s038-iit-madras/UDIT_DL_1</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250309_182609-ljxmi4ab/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuGIytQQLYHV"
      },
      "source": [
        "###Predictions and accuracy using validation data and test data\n",
        "(Using best identified model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV2-vMJ4igo4",
        "outputId": "b87ee3bb-01dc-4448-c599-8aa1140beb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------epoch 0-----------------\n",
            "Training loss:  [0.77735281]\n",
            "Validation accuracy:  0.8158333333333333\n",
            "Validation loss:  [9.89928489]\n",
            "-----------------epoch 1-----------------\n",
            "Training loss:  [0.46647724]\n",
            "Validation accuracy:  0.831\n",
            "Validation loss:  [10.21238078]\n",
            "-----------------epoch 2-----------------\n",
            "Training loss:  [0.42319819]\n",
            "Validation accuracy:  0.8416666666666667\n",
            "Validation loss:  [10.42410541]\n",
            "-----------------epoch 3-----------------\n",
            "Training loss:  [0.40051628]\n",
            "Validation accuracy:  0.8445\n",
            "Validation loss:  [10.58235288]\n",
            "-----------------epoch 4-----------------\n",
            "Training loss:  [0.38502958]\n",
            "Validation accuracy:  0.8505\n",
            "Validation loss:  [10.71554324]\n",
            "-----------------epoch 5-----------------\n",
            "Training loss:  [0.37317243]\n",
            "Validation accuracy:  0.8548333333333333\n",
            "Validation loss:  [10.83665818]\n",
            "-----------------epoch 6-----------------\n",
            "Training loss:  [0.363549]\n",
            "Validation accuracy:  0.8566666666666667\n",
            "Validation loss:  [10.95442402]\n",
            "-----------------epoch 7-----------------\n",
            "Training loss:  [0.35544799]\n",
            "Validation accuracy:  0.859\n",
            "Validation loss:  [11.07378557]\n",
            "-----------------epoch 8-----------------\n",
            "Training loss:  [0.34845758]\n",
            "Validation accuracy:  0.8605\n",
            "Validation loss:  [11.19673592]\n",
            "-----------------epoch 9-----------------\n",
            "Training loss:  [0.34225044]\n",
            "Validation accuracy:  0.8626666666666667\n",
            "Validation loss:  [11.32261043]\n",
            "----------------------------------\n",
            "Test accuracy:  0.8616\n",
            "Test loss:  [11.88034076]\n"
          ]
        }
      ],
      "source": [
        "activation = 'tanh'\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "h_layers = 4\n",
        "learning_rate = 0.0001\n",
        "neurons = 128\n",
        "optimizer = 'adam'\n",
        "\n",
        "output_test = model_train(epochs, learning_rate, neurons, h_layers, activation, batch_size, optimizer, x_train, y_train, x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QqPFVKyXuT_D"
      },
      "outputs": [],
      "source": [
        "# predict(input, y, layers):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC4-NOtfn9wV"
      },
      "source": [
        "###Confusion Matrix with best model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsJrAaGV3Wt_"
      },
      "outputs": [],
      "source": [
        "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                        y_true=y_test, preds=output_test,\n",
        "                        class_names=labels)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EB3zv981UERv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}